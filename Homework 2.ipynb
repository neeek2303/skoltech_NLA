{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 2 (145 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (LU decomposition) 25 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LU for band matrices (7 pts)\n",
    "\n",
    "The complexity to find an LU decomposition of a dense $n\\times n$ matrix is $\\mathcal{O}(n^3)$.\n",
    "Significant reduction in complexity can be achieved if the matrix has a certain structure, e.g. it is sparse. \n",
    "In the following task we consider an important example of $LU$ for a special type of sparse matrices –– band matrices with the bandwidth $m$ equal to 3 or 5 which called tridiagonal and pentadiagonal respectively.\n",
    "\n",
    "- (5 pts) Write a function ```band_lu(diag_broadcast, n)``` which computes LU decomposition for tridiagonal or pentadiagonal matrix with given diagonal values. \n",
    "For example, input parametres ```(diag_broadcast = [4,-2,1], n = 4)``` mean that we need to find LU decomposition for the triangular matrix of the form:\n",
    "\n",
    "$$A = \\begin{pmatrix}\n",
    "-2 & 1 & 0 & 0\\\\\n",
    "4 & -2 & 1 & 0 \\\\\n",
    "0 & 4 & -2 & 1 \\\\\n",
    "0 & 0 & 4 & -2 \\\\\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "As an output it is considered to make ```L``` and ```U``` - 2D arrays representing diagonals in factors $L$ (```L[0]``` keeps first lower diagonal, ```L[1]``` keeps second lower, ...), and $U$ (```U[:,0]``` keeps main diagonal, ```U[:,1]``` keeps first upper, ...).\n",
    "- (2 pts) Compare execution time of the band LU decomposition using standard function from ```scipy```, i.e. which takes the whole matrix and does not know about its special structure, and band decomposition of yours implementation. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import diags\n",
    "from scipy.linalg import lu\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = diags([1,2,20,3,4], [-2,-1, 0, 1, 2], shape=(5,5)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, U = lu(A, permute_l=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.,  3.,  4.,  0.,  0.],\n",
       "       [ 2., 20.,  3.,  4.,  0.],\n",
       "       [ 1.,  2., 20.,  3.,  4.],\n",
       "       [ 0.,  1.,  2., 20.,  3.],\n",
       "       [ 0.,  0.,  1.,  2., 20.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.1       , 1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.05      , 0.09390863, 1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.05076142, 0.09552239, 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.05113563, 0.09545563, 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.        ,  3.        ,  4.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , 19.7       ,  2.6       ,  4.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , 19.55583756,  2.62436548,  4.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , 19.54626866,  2.61791045],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        , 19.5455632 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_lu(diag, n):\n",
    "    n_d = len(diag)\n",
    "    if n_d==3:\n",
    "        diagonals = [[diag[0] for i in range(n-1)],[diag[1] for i in range(n)],[diag[2] for i in range(n-1)]]\n",
    "        a = diagonals[0]\n",
    "        b = diagonals[1] \n",
    "        c = diagonals[2] \n",
    "        n = len( b )\n",
    "        for i in range( 1, n ):\n",
    "            a[i-1] = a[i-1] / b[i-1]\n",
    "            b[i] = b[i] - a[i-1] * c[i-1]\n",
    "        U = [b,c]\n",
    "        L = [a]\n",
    "    elif n_d==5:\n",
    "        diagonals=[[diag[0] for i in range(n-2)],[diag[1] for i in range(n-1)],[diag[2] for i in range(n)],[diag[3] for i in range(n-1)],[diag[4] for i in range(n-2)]]\n",
    "        a = diagonals[4] \n",
    "        b = diagonals[3] \n",
    "        c = diagonals[2] \n",
    "        d = diagonals[1] \n",
    "        e = diagonals[0] \n",
    "        \n",
    "        n = len( c )\n",
    "        a = [0]+[0]+[0]+a\n",
    "        b = [0]+[0]+b\n",
    "        c = [0]+c\n",
    "        d = [0]+d\n",
    "        e = [0]+e\n",
    "        l = len(c)\n",
    "        a_new, b_new, c_new, d_new, e_new =[0]*l, [0]*l, [0]*l, [0]*l, [0]*l \n",
    "        b_new[2] = b[2]\n",
    "        c_new[1] = c[1]\n",
    "        d_new[1] =-d[1]/c_new[1]\n",
    "        e_new[1] = -e[1]/c_new[1]\n",
    "        c_new[2] = c[2]+b[2]*d_new[1]\n",
    "        d_new[2] = -(d[2]+b[2]*e_new[1])/c_new[2]\n",
    "        e_new[2] = -e[2]/c_new[2]\n",
    "        for i in range(3, n+1):\n",
    "            c_new[i] = c[i] + (a[i]*d_new[i-2]+b[i])*d_new[i-1]+a[i]*e_new[i-2]\n",
    "            if i<=n-1:\n",
    "                d_new[i] = -(d[i]+(a[i]*d_new[i-2]+b[i])*e_new[i-1])/c_new[i]\n",
    "            if i<=n-2:    \n",
    "                e_new[i] = -e[i]/c_new[i]\n",
    "            b_new[i] = a[i]*d_new[i-2]+b[i]\n",
    "            a_new[i] = a[i]\n",
    "        d_new = [-i for i in d_new]\n",
    "        e_new = [-i for i in e_new]\n",
    "        U = [c_new[1:], b_new[2:], a_new[3:] ]\n",
    "        L = [d_new[1:-1], e_new[1:-2]]\n",
    "    else:\n",
    "        raise NotImplementedError() \n",
    "    return L, U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "digs = [2,20,3]\n",
    "l = 5\n",
    "\n",
    "n = len(digs)\n",
    "_L, _U = band_lu(digs,l) \n",
    "ones = [1]*l\n",
    "\n",
    "L = diags([ones]+[i for i in _L], [i for i in range(0,-(n+1)//2,-1)]).toarray()\n",
    "U = diags([i for i in _U],  [i for i in range(0,(n+1)//2,1)]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.        ,  3.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , 19.7       ,  3.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        , 19.69543147,  3.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , 19.69536082,  3.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        , 19.69535973]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.1       , 1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.10152284, 1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.10154639, 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.10154676, 1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.,  3.,  0.,  0.,  0.],\n",
       "       [ 2., 20.,  3.,  0.,  0.],\n",
       "       [ 0.,  2., 20.,  3.,  0.],\n",
       "       [ 0.,  0.,  2., 20.,  3.],\n",
       "       [ 0.,  0.,  0.,  2., 20.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L@U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "band_lu = 0.008013800000014726\n",
      "scipy = 0.055444500000021435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neeek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\neeek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "diag_broadcast =[1,2,20,3,4]\n",
    "n = 1000\n",
    "matrix = diags(diag_broadcast, [-2,-1, 0, 1, 2], shape=(n,n)).toarray()\n",
    "L, U = band_lu(diag_broadcast, n)\n",
    "print('band_lu =', time.perf_counter() - start_time)\n",
    "\n",
    "start_time1 = time.clock()\n",
    "p, l, u = lu(matrix)\n",
    "print('scipy =', time.perf_counter() - start_time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy takes more time because its make all multiplications, our algorithm do only necessary multiplications - differentiantion is huge if matrix is big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stability of LU (8 pts)\n",
    "\n",
    "Let\n",
    "$A = \\begin{pmatrix}\n",
    "\\varepsilon & 1 & 0\\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}.$ \n",
    "* (5 pts) Find analytically LU decomposition with and without pivoting for the matrix $A$.\n",
    "* (3 pts) Explain, why can the LU decomposition fail to approximate factors $L$ and $U$ for $|\\varepsilon|\\ll 1$ in computer arithmetic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "if $\\varepsilon = 0$ we don't have LU decomposition because without pivoting A = LU, L -  unit lower triangular, U -  upper triangular. A should be strictly regular.\n",
    "\n",
    "$\n",
    "L =\n",
    "=  \\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "l_1 & 1 & 0 \\\\\n",
    "0 & l_2 & 1\n",
    "\\end{pmatrix}\\\\\n",
    "$\n",
    "\n",
    "$\n",
    "U =\n",
    "\\begin{pmatrix}\n",
    "u_1 & 1 & 0\\\\\n",
    "0 & u_2 & 1 \\\\\n",
    "0 & 0 & u_3\n",
    "\\end{pmatrix}    $ \n",
    "\n",
    "\n",
    "$\\varepsilon \\neq 0$\n",
    "\n",
    "\n",
    "\n",
    "$ \\varepsilon = u_1 $\n",
    "\n",
    "$l_1 = \\frac{1}{\\varepsilon}$\n",
    "\n",
    "$l_1 + u_2 = 1 \\rightarrow u_2 = 1 - \\frac{1}{\\varepsilon} = \\frac{\\varepsilon - 1  }{\\varepsilon} $\n",
    "\n",
    "$l_2 u_2 = 1 \\rightarrow l_2 = \\frac{\\varepsilon  }{\\varepsilon -1} $\n",
    "\n",
    "$l_2 + u_3 = 1 \\rightarrow u_3 = \\frac{-1 }{\\varepsilon -1} $\n",
    "\n",
    "if $\\varepsilon = 1$, we get division by zero. So $\\varepsilon \\neq  1$\n",
    "\n",
    "if $\\varepsilon -> 0$, that $u_2  -> \\infty $ and $l_1 - > \\infty $\n",
    "\n",
    "$ \n",
    "L =\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "\\frac{1}{\\varepsilon} & 1 & 0 \\\\\n",
    "0 & \\frac{\\varepsilon  }{\\varepsilon -1} & 1\n",
    "\\end{pmatrix} \\\\\n",
    "$\n",
    "\n",
    "$\n",
    "U = \n",
    "\\begin{pmatrix}\n",
    "\\varepsilon  & 1 & 0\\\\\n",
    "0 & \\frac{\\varepsilon - 1  }{\\varepsilon} & 1 \\\\\n",
    "0 & 0 &  \\frac{-1 }{\\varepsilon -1}\n",
    "\\end{pmatrix}     $ \n",
    "\n",
    "with pivoting: A = PLU, P - is a permutation matrix, PLU factorization is not unique, there are a lot of different P, L, U\n",
    "\n",
    "\n",
    "$ \n",
    "P^{-1} A = \\begin{pmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "\\varepsilon & 1 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "l_1 & 1 & 0 \\\\\n",
    "0 & l_2 & 1\n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}\n",
    "u_1 & m_0 & 1\\\\\n",
    "0 & u_2 & m_1 \\\\\n",
    "0 & 0 & u_3\n",
    "\\end{pmatrix} =  \\begin{pmatrix}\n",
    "u_1 & m_0 + u_1 & 1\\\\\n",
    "l_1 u_0  & l_1 m_0 + u_1 & l_1 + m_1 \\\\\n",
    "0 & l_2 u_1 & l_2 m_1 + u_2\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "u_1 & m_0 + u_1 & 1\\\\\n",
    "l_1 u_0  & l_1 m_0 + u_1 & l_1 + m_1 \\\\\n",
    "0 & l_2 u_1 & l_2 m_1 + u_2\n",
    "\\end{pmatrix} $ \n",
    "\n",
    "\n",
    "$ P = \\begin{pmatrix}\n",
    "0 & 1 & 0\\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix} $\n",
    "\n",
    "$P^{-1} A =  \\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "\\varepsilon & 1 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{pmatrix}  \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 1\\\\\n",
    "0 & 1 & -\\varepsilon \\\\\n",
    "0 & 0 & 1+ \\varepsilon\n",
    "\\end{pmatrix} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Block LU (10 pts)\n",
    "\n",
    "Let $A = \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix}$ be a block matrix. The goal is to solve the linear system\n",
    "\n",
    "$$\n",
    "     \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "* (2 pts) Using block elimination find matrix $S$ and right-hand side $\\hat{f_2}$ so that $u_2$ can be found from $S u_2 = \\hat{f_2}$. Note that the matrix $S$ is called <font color='red'> Schur complement </font> of the block $A_{11}$.\n",
    "* (4 pts) Using Schur complement properties prove that \n",
    "\n",
    "$$\\det(X+AB) = \\det(X)\\det(I+BX^{-1}A), $$\n",
    "\n",
    "\n",
    "where $X$ - nonsingular square matrix.\n",
    "* (4 pts) Let matrix $F \\in \\mathbb{R}^{m \\times n}$ and $G \\in \\mathbb{R}^{n \\times m}$. Prove that \n",
    "\n",
    "$$\\det(I_m - FG) = \\det(I_n - GF).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix}\n",
    "\\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix} $\n",
    "\n",
    "$ \\begin{bmatrix} \n",
    "A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} = \\begin{bmatrix} \n",
    "L_{11} & 0 \\\\ L_{21} & L_{22} \\end{bmatrix} \\begin{bmatrix} \n",
    "U_{11} & U_{12} \\\\ 0 & U_{22} \\end{bmatrix} = \\begin{bmatrix} \n",
    "L_{11} U_{11} & L_{11} U_{12} \\\\ L_{21} U_{11} & L_{21} U_{12} + L_{22} U_{22} \\end{bmatrix} $\n",
    "\n",
    "$U_{12} = L^{-1}_{11} A_{12}$\n",
    "\n",
    "$L_{21} = A_{21} U^{-1}_{11} $\n",
    "\n",
    "$ L_{22} U_{22} = A_{22} - L_{21} U_{12} = A_{22} - A_{21} U^{-1}_{11} L^{-1}_{11} A_{12} = A_{22} - A_{21} A^{-1}_{11} A_{12} = S$\n",
    "\n",
    "$ \\hat{f_2} = f_2 - A_{21} A^{-1}_{11} f_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 \n",
    "\n",
    "$M = \\begin{bmatrix} \n",
    "A & B \\\\ C & D \\end{bmatrix} = \\begin{bmatrix} \n",
    "I & 0 \\\\ CA^{-1} & I \\end{bmatrix} \\begin{bmatrix} \n",
    "A & 0 \\\\ 0 & D-C^{-1}AB \\end{bmatrix} \\begin{bmatrix} \n",
    "I & A^{-1}B \\\\ 0 & I \\end{bmatrix} = \\begin{bmatrix} \n",
    "I & BD^{-1} \\\\ 0 & I \\end{bmatrix} \\begin{bmatrix} \n",
    "A-BD^{-1}C & 0 \\\\0 & D \\end{bmatrix} \\begin{bmatrix} \n",
    "I & 0 \\\\ D^{-1}C & I \\end{bmatrix} $\n",
    "\n",
    "$det(M) = det(D)det(A - B D^{-1}C) = det(A)det(D - C A^{-1}B) $\n",
    "\n",
    "Let's $ M = \\begin{bmatrix} \n",
    "X & A \\\\ -B & I \\end{bmatrix} = \\det(X+AB) = \\det(X)\\det(I+BX^{-1}A) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 \n",
    "just use previous formula for\n",
    "$M = \\begin{bmatrix} \n",
    "I_m & F \\\\ G & I_n \\end{bmatrix} $ => $det(M) = det(I_n)det(I_m - F G) = det(I_m)det(I_n - GF) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Problem 2 (QR decomposition) 30 pts\n",
    "\n",
    "### 1. Standard Gram-Schmidt algorithm (18 pts)\n",
    "Our goal is to orthogonalize a system of linearly independent vectors $v_1,\\dots,v_n$.\n",
    "The standard algorithm for this task is the Gram-Schmidt process:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "u_1 &= v_1, \\\\\n",
    "u_2 &= v_2 - \\frac{(v_2, u_1)}{(u_1, u_1)} u_1, \\\\\n",
    "\\dots \\\\\n",
    "u_n &= v_n - \\frac{(v_n, u_1)}{(u_1, u_1)} u_1 - \\frac{(v_n, u_2)}{(u_2, u_2)} u_2 - \\dots - \\frac{(v_n, u_{n-1})}{(u_{n-1}, u_{n-1})} u_{n-1}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Obtained $u_1, \\dots, u_n$ are orthogonal vectors in exact arithmetics. Then to make the system orthonormal you should divide each of the vectors by its norm: $u_i := u_i/\\|u_i\\|$.\n",
    "The Gram-Schmidt process can be considered as a QR decomposition. Let us show that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2 pts) Write out what is matrices $Q$ and $R$ obtained in the process above. \n",
    "\n",
    "* (5 pts) Implement the described Gram-Schmidt algorithm as a function ```gram_schmidt_qr(A)``` that takes a rectangular matrix ```A``` and outputs ```Q,R```.\n",
    "\n",
    "* (3 pts) Create a rectangular matrix $K = [x, Ax, A^2x, \\ldots, A^kx]$ where $A \\in \\mathbb{R}^{100 \\times 100}$ is a random matrix and $x$ is a random vector of the appropriate dimension, $k = 20$.  \n",
    "The loss of orthogonality can be described by the following error: $\\|Q^{\\top}Q-I\\|_2$, where $Q^{\\top}Q$ is called a Gram matrix. Compute QR decomposition of the created matrix $K$ with function that you have implemented and calculate error $\\|Q^{\\top}Q-I\\|_2$. Comment on the result.\n",
    "\n",
    "* (5 pts) The observed loss of orthogonality is a problem of this particular algorithm. Luckily, there is [a simple improvement to the algorithm above](https://en.wikipedia.org/wiki/Gram–Schmidt_process#Numerical_stability) that reduces the loss of orthogonality. Implement this modification as a function ```modified_gram_schmidt_qr(A)``` such that input and output are similar to ```gram_schmidt_qr(A)```. \n",
    "* (3 pts) Compute QR decomposition of the matrix $K$ from the previous task with the function ```modified_gram_schmidt_qr(A)```.\n",
    "Compute error $\\|Q^{\\top}Q-I\\|_2$. Compare this error to the error obtained with a \"pure\" Gram-Schmidt and comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$Q$ is an orthogonal matrix and $R$ is an upper triangular matrix.\n",
    "\n",
    "$ Q = [\\frac{u_1}{\\|u_1\\|}, ..., \\frac{u_n}{\\|u_n\\|}] $\n",
    "\n",
    "$R$ - is a matrix of transition matrix, $R = Q^T A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt_qr(A):\n",
    "    \n",
    "    Q = []\n",
    "    A = A.transpose()\n",
    "    m = A.copy()\n",
    "    u = A\n",
    "\n",
    "    summ = 0\n",
    "    for i in range(1, A.shape[0]):\n",
    "        for j in range(0, i):\n",
    "            summ = summ + (np.dot(A[i], u[j])/np.dot(u[j],u[j]))*u[j]\n",
    "        for s in range(0, i):\n",
    "            u[i] = u[i] - summ\n",
    "        summ = 0\n",
    "            \n",
    "    for k in range(u.shape[0]):\n",
    "        Q.append(u[k]/(np.linalg.norm(u[k], ord=2)))\n",
    "    \n",
    "    R = np.dot(Q, m.transpose())\n",
    "    \n",
    "    return np.transpose(Q), R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[12, -51], [6, 167], [-4, 24]])\n",
    "Q, R = gram_schmidt_qr(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85714286, -0.39428571],\n",
       "       [ 0.42857143,  0.90285714],\n",
       "       [-0.28571429,  0.17142857]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.40000000e+01,  2.10000000e+01],\n",
       "       [-6.66133815e-16,  1.75000000e+02]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import matrix_power as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "_K = [mp(A,k)@x for k in range(21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.hstack(_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 21)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.748614791407453"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(0,1, 20)\n",
    "# V = np.vander(x, 20)\n",
    "    \n",
    "Q_v, R_v = gram_schmidt_qr(K)\n",
    "I = np.identity(21)\n",
    "np.linalg.norm(np.dot(np.transpose(Q_v), Q_v) - I, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_gram_schmidt_qr(A):\n",
    "    \n",
    "    Q = []\n",
    "    A = A.transpose()\n",
    "    m = A.copy()\n",
    "    u = A\n",
    "    \n",
    "    for i in range(1, A.shape[0]):\n",
    "        for j in range(0, i):\n",
    "            r = np.dot(A[i], u[j])/np.dot(u[j],u[j])\n",
    "            u[i] = u[i] - r*u[j]\n",
    "    \n",
    "    for k in range(u.shape[0]):\n",
    "        Q.append(u[k]/(np.linalg.norm(u[k], ord=2)))\n",
    "        \n",
    "    R = np.dot(Q, m.transpose())\n",
    "    \n",
    "    return np.transpose(Q), R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3204481674065137e-14"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_m, R_m = modified_gram_schmidt_qr(K)\n",
    "np.linalg.norm(np.dot(np.transpose(Q_m), Q_m) - I, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified gram schmidt_qr provide better elementwise orthogonality. Lose of orthogonality increase at every step at gram schmidt qr,  but at modified gram schmidt qr it is improved, because we do less orthogonalization operations at every step(now we do it only one time) and thus dont accumulate erroe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Updating QR decompositions (12 pts)\n",
    "\n",
    "Suppose we are dealing with overdetermined system:\n",
    "\n",
    "$$ Ax = y, \\;\\; A \\in \\mathbb{R}^{m \\times n}, \\;\\;  m \\geq n. $$\n",
    "\n",
    "If you were attentive during the course, you should be aware of \n",
    "the canonical way to solve this system. \n",
    "\n",
    "- No, implementing **Householder $QR$** will not be your task :)\n",
    "\n",
    "- We assume that $QR$ decomposition of $A$ **is given**:\n",
    "\n",
    "$$ QR = A = \\begin{bmatrix}a_1 & a_2 & \\ldots & a_n \\end{bmatrix}, \\;\\; a_i \\in \\mathbb{R}^m,$$\n",
    "\n",
    "where $Q \\in \\mathbb{R}^{m \\times m}$ is unitary matrix and $R \\in \\mathbb{R}^{m \\times n}$ is upper triangular.\n",
    "\n",
    "- Let $k$ be an arbitrary column index, $1 \\leq k \\leq n$, and $z \\in \\mathbb{R}^{m}$ be an arbitrary vector.\n",
    "\n",
    "We are interested in matrices:\n",
    "\n",
    "$$ \\tilde{A} = \\begin{bmatrix}a_1 & \\ldots & a_{k-1} & a_{k+1} & \\ldots a_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times (n-1)},$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$ \\hat{A} = \\begin{bmatrix}a_1 & \\ldots & a_{k} & z & a_{k+1} & \\ldots a_n \\end{bmatrix} \\in \\mathbb{R}^{m \\times (n+1)},$$\n",
    "\n",
    "which are obtained by deleting / inserting one column from / to the matrix $A$.\n",
    "\n",
    "For example, if $A$ comes from a least-squares problem, such matrices appear if one wants to examine the significance of the $k^{th}$ parameter in the underlying model (e.g. think in terms of linear regression).\n",
    "\n",
    "- We want to compute decompostions of these updated matrices:\n",
    "\n",
    "$$ \\tilde{A} = \\tilde{Q}\\tilde{R}, \\;\\; \\hat{A} = \\hat{Q}\\hat{R}. $$\n",
    "\n",
    "Performing full orthogonalization process will be redundant. Instead, the known $QR$ factors of $A$ should be **updated** in a proper way.\n",
    "\n",
    "- Different methods for performing $QR$ decomposition were studied during the course. We expect the factors of updated matrices can be recovered with a small number of selective updates, hence the **Givens rotations** seem to be the most appropriate.\n",
    "\n",
    "- Recall the Givens rotation matrix :\n",
    "\n",
    "$$G_{ij}(\\theta) = \\begin{bmatrix} \n",
    "1      & \\ldots & 0      & \\ldots & 0      & \\ldots & 0      \\\\\n",
    "\\vdots & \\ddots & \\vdots &        & \\vdots &        & \\vdots \\\\\n",
    "0      & \\ldots & c      & \\ldots & s      & \\ldots & 0      \\\\\n",
    "\\vdots &        & \\vdots & \\ddots & \\vdots &        & \\vdots \\\\\n",
    "0      & \\ldots & -s     & \\ldots & c      & \\ldots & 0      \\\\\n",
    "\\vdots &        & \\vdots &        & \\vdots & \\ddots & \\vdots \\\\\n",
    "0      & \\ldots & 0      & \\ldots & 0      & \\ldots & 1      \n",
    "\\end{bmatrix}  \\;\\; \\in \\mathbb{R}^{m \\times m}, \\;\\; c = \\cos \\theta, \\;\\; s = \\sin \\theta,$$ \n",
    "\n",
    "which differs from the identity $\\mathrm{I}^{m \\times m}$ by a $2 \\times 2$ sub-matrix:\n",
    "\n",
    "$$\n",
    "{G}(\\theta) = \\begin{bmatrix} c & s \\\\ -s & c \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 2},\n",
    "$$\n",
    "placed on the rows and columns with indices $i$ and $j$. \n",
    "\n",
    "1. (1 pts)\n",
    "Matrix ${G}$ has clear geometric interpretation in 2 dimensions. Let  $a = \\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix} \\in \\mathbb{R}^2$ be an arbitrary non-zero vector. According to the template provided below, implement the function to construct matrix $G$, such that:\n",
    "\n",
    "$$ {G}^T \\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix}.$$\n",
    "\n",
    "What are geometrical meanings of $\\alpha$ and $\\theta$? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.67484754e-01, -1.05171695e-17])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def givens(a):\n",
    "    '''\n",
    "    Input: \n",
    "        a: np.array of size (2)\n",
    "    Output: \n",
    "        G: np.array of size (2, 2)\n",
    "            Rotation matrix, G^T eliminates the second component of a\n",
    "    '''\n",
    "    x = np.sqrt(a[0]**2+a[1]**2)\n",
    "    c = a[0]/x\n",
    "    s = -a[1]/x\n",
    "    return np.array([[c,s],[-s,c]])\n",
    "# Check yourself \n",
    "a = np.random.rand(2)\n",
    "givens(a).T.dot(a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\alpha$  is length of vector $\\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix}$ : $\\alpha = \\sqrt{a_1^2+a_2^2}$\n",
    "### $\\theta$ is angle of rotation such that $c = cos(\\theta), s = sin(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarly, matrix $G_{ij}$ can be constructed in order to eliminate the $j^{th}$ component of an arbitrary vector of size $m$. This leads to the well-known **Givens $QR$ algorithm** (see lecture notes!).\n",
    "\n",
    "- Givens rotations typically appear in matrix form:\n",
    "\n",
    "$$ G_{ij}^T X, \\;\\; X^{'}G_{ij},$$\n",
    "\n",
    "where $X$ and $X^{'}$ are some matrices with compatible sizes, e.g. intermediate factors in the **Givens $QR$ algorithm**. \n",
    "\n",
    "How to perform these multiplications efficiently? \n",
    "Obviously, $G_{ij}$ does not need to be stored in explicit form. It affects only two rows / columns, hence only appropriate submatrices should be modified.\n",
    "\n",
    "2. (5 pts) Your second task is to propose an efficient procedure for computing $\\tilde{Q}, \\tilde{R}$ factors of $\\tilde{A}$ by a proper update of $Q, R$ factors of $A$. \n",
    "\n",
    "Hint: consider the following block partition of $R$ (assuming $k^{th}$ column deleted):\n",
    "$$ R = \n",
    "\\begin{bmatrix} \n",
    "R_{11} & v & R_{13} \\\\\n",
    "0 & r_{kk} & \\omega^T \\\\\n",
    "0 & 0 & R^{33}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "and note, that:\n",
    "$$Q^T\\tilde{A} = \n",
    "\\begin{bmatrix}\n",
    "R_{11} & R_{13} \\\\\n",
    "0 & \\omega^T \\\\\n",
    "0 & R_{33}\n",
    "\\end{bmatrix} = H.$$\n",
    "\n",
    "- (2 pts) What structure has matrix $H$ and how does it help in understanding the proper way to compute updated QR decomposition?\n",
    "\n",
    "- (3 pts) Implement the function according to the template provided below.\n",
    "Use Givens matrices $G_{ij}$ with $j = i+1$ for the convinience.\n",
    "\n",
    "- Only one loop over non-zero subdiagonal elements of $H$ is allowed. You are supposed to perform computations in a proper vectorized form. For example, the *in-place* multiplication $G_{ij}^T X$ with a random matrix $X \\in \\mathbb{R}^{m \\times n}$ can be implemented as follows: ```X[[i,j],:] = G.T.dot(X[[i,j],:])```. \n",
    "\n",
    "- Do not modify elements which are known to be zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.979106443895821e-14\n"
     ]
    }
   ],
   "source": [
    "def delete_column_QR(A, k):\n",
    "    '''\n",
    "    Input: \n",
    "        A: np.array of size (m, n) \n",
    "            \n",
    "        k: positive integer\n",
    "            Index of deleted column, 0 \\leq k \\leq n - 1\n",
    "        \n",
    "    Output: \n",
    "        Q: np.array of size (m, m)\n",
    "        \n",
    "        R: np.array of size (m, n - 1)\n",
    "\n",
    "    '''\n",
    "    Q, R = np.linalg.qr(A, mode='complete') \n",
    "    R = np.delete(R, k, axis = 1)\n",
    "\n",
    "    for i in range(n-k-1):\n",
    "        giv = givens(R[i:i+2,i])\n",
    "        R[[i,i+1],:] = giv.T@R[[i,i+1],:]\n",
    "        Q[:, [i,i+1]] = Q[:,[i,i+1]]@giv\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "# Check yourself\n",
    "m, n = (200, 100)\n",
    "A = np.random.rand(m, n)\n",
    "k = np.random.randint(0, n)\n",
    "A_tilde = np.delete(A, k, axis=1)\n",
    "Q_tilde, R_tilde = delete_column_QR(A, k)\n",
    "print(np.linalg.norm(A_tilde - Q_tilde.dot(R_tilde)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (3 pts) When a column is inserted, the factors $\\hat{Q}, \\hat{R}$ of $\\hat{A}$ again can be computed efficiently by a proper update of $Q, R$ factors of $A$.\n",
    "\n",
    "\n",
    "- Proceed similarly to the previous task. Implement the function according to the template provided below. \n",
    "\n",
    "- Choose optimal order of elimination to minimize the number of rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.760423708088214e-14\n"
     ]
    }
   ],
   "source": [
    "def insert_column_QR(A, k, z):\n",
    "    '''\n",
    "    Input: \n",
    "        A: np.array of size (m, n) \n",
    "            \n",
    "        k: positive integer\n",
    "            Index of inserted column, 0 \\leq k leq n - 1\n",
    "            \n",
    "        z: np.array of size (m)\n",
    "            Inserted column\n",
    "        \n",
    "    Output: \n",
    "        Q: np.array of size (m, m)\n",
    "        \n",
    "        R: np.array of size (m, n + 1)\n",
    "    '''\n",
    "    Q, R = np.linalg.qr(A, mode='complete')\n",
    "    R = np.insert(R, k, Q.T@z, axis=1)\n",
    "    m = R.shape[0]\n",
    "    \n",
    "    for i in range(m-1,k, -1):\n",
    "        giv = givens(R[[i-1,i],k])\n",
    "        R[[i-1,i],:] = giv.T@R[[i-1,i],:]\n",
    "        Q[:, [i-1,i]] = Q[:, [i-1,i]]@giv\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "# Check yourself\n",
    "m, n = (200, 100)\n",
    "A = np.random.rand(m, n)\n",
    "k = np.random.randint(0, n)\n",
    "z = np.random.rand(m)\n",
    "A_hat = np.insert(A, k, z, axis=1)\n",
    "Q_hat, R_hat = insert_column_QR(A, k, z)\n",
    "print(np.linalg.norm(A_hat - Q_hat.dot(R_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (3 pts) In the context of least-squares problems, appending rows to a matrix is just as important as inserting columns (e.g. new samples in linear regression).\n",
    "\n",
    "Consider matrix:\n",
    "\n",
    "$$ \\overline{A} = \\begin{bmatrix} \\omega^T \\\\ A \\end{bmatrix} \\in \\mathbb{R}^{(m+1) \\times n},$$\n",
    "\n",
    "which is obtained by appending a row $\\omega^T \\in \\mathbb{R}^{n}$ to the matrix $A$.\n",
    "\n",
    "Similarly to the previous tasks, you need to compute $\\overline{Q}, \\overline{R}$ factors of $\\overline{A}$ by a proper update of $Q,R$ factors of $A$.\n",
    "\n",
    "- Implement the function according to the template provided below. Your code is still expected to be efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.392711818923361e-14\n"
     ]
    }
   ],
   "source": [
    "def insert_row_QR(A, w):\n",
    "    '''\n",
    "    Input: \n",
    "        A: np.array of size (m, n)\n",
    "\n",
    "        w: np.array of size (n)\n",
    "            Inserted row\n",
    "        \n",
    "    Output:\n",
    "        Q: np.array of size (m + 1, m + 1)\n",
    "        \n",
    "        R: np.array of size (m + 1, n)\n",
    "    '''\n",
    "    Q, R = np.linalg.qr(A, mode='complete') \n",
    "    R = np.insert(R, 0, w, axis=0)\n",
    "    Q = np.insert(Q, 0, np.array([0]*m), axis=1)\n",
    "    Q = np.insert(Q, 0, np.array([0]*(m+1)), axis=0)\n",
    "    Q[0,0] = 1\n",
    "   \n",
    "    for i in range(0,n-1):\n",
    "        giv = givens(R[i:i+2,i])\n",
    "        R[[i,i+1],:] = giv.T@R[[i,i+1],:]\n",
    "        Q[:, [i,i+1]] = Q[:,[i,i+1]]@giv\n",
    "    return Q, R\n",
    "\n",
    "# Check yourself\n",
    "m, n = (200, 100)\n",
    "A = np.random.rand(m, n)\n",
    "w = np.random.rand(n)\n",
    "A_overline = np.insert(A, 0, w, axis=0)\n",
    "Q_overline, R_overline = insert_row_QR(A, w)\n",
    "print(np.linalg.norm(A_overline - Q_overline.dot(R_overline)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (Word2Vec as Matrix Factorization) 45 pts\n",
    "\n",
    "In this assignment you are supposed to apply SVD to training your own [word embedding model](https://en.wikipedia.org/wiki/Word_embedding) which maps English words to vectors of real numbers.\n",
    "\n",
    "Skip-Gram Negative Sampling (SGNS) word embedding model, commonly known as **word2vec** ([Mikolov et al., 2013](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)), is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as implicit matrix factorization objective as was shown in ([Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Notation\n",
    "Assume we have a text corpus given as a sequence of words $\\{w_1,w_2,\\dots,w_n\\}$ where $n$ may be larger than $10^{12}$ and $w_i \\in \\mathcal{V}$ belongs to a vocabulary of words $\\mathcal{V}$. A word $c \\in \\mathcal{V}$ is called *a context* of word $w_i$ if they are found together in the text. More formally, given some measure $L$ of closeness between two words (typical choice is $L=2$), a word $c \\in \\mathcal{V}$ is called a context if $c \\in \\{w_{i-L}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+L} \\}$ Let $\\mathbf{w},\\mathbf{c}\\in\\mathbb{R}^d$ be the *word embeddings* of word $w$ and context $c$, respectively. Assume they are specified by the mapping  $\\Phi:\\mathcal{V}\\rightarrow\\mathbb{R}^d$, so $\\mathbf{w}=\\Phi(w)$. The ultimate goal of SGNS word embedding model is to fit a good mapping $\\Phi$.\n",
    "\n",
    "Let $\\mathcal{D}$ be a multiset of all word-contexts pairs observed in the corpus. In the SGNS model, the probability that word-context pair $(w,c)$ is observed in the corpus is modeled as the following distribution:\n",
    "\n",
    "$$\n",
    "P(\\#(w,c)\\neq 0|w,c) = \\sigma(\\mathbf{w}^\\top \\mathbf{c}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^\\top \\mathbf{c})},\n",
    "$$\n",
    "\n",
    "where $\\#(w,c)$ is the number of times the pair $(w,c)$ appears in $\\mathcal{D}$ and $\\mathbf{w}^\\top\\mathbf{c}$ is the scalar product of vectors $\\mathbf{w}$ and $\\mathbf{c}$. Two important quantities which we will also use further are the number of times the word $w$ and the context $c$ appear in $\\mathcal{D}$, which can be computed as\n",
    "\n",
    "$$\n",
    "\\#(w) = \\sum_{c\\in\\mathcal{V}} \\#(w,c), \\quad \\#(c) = \\sum_{w\\in\\mathcal{V}} \\#(w,c).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Optimization objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla word embedding models are trained by maximizing log-likelihood of observed word-context pairs, namely\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram Negative Sampling approach modifies the objective by additionally minimizing the log-likelihood of random word-context pairs, so called *negative samples*. This idea incorporates some useful linguistic information that some number ($k$, usually $k=5$) of word-context pairs *are not* found together in the corpus which usually results in word embeddings of higher quality. The resulting optimization problem is\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) + k \\cdot \\mathbb{E}_{c'\\sim P_\\mathcal{D}} \\log \\sigma (-\\mathbf{w}^\\top\\mathbf{c}) \\right) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d},\n",
    "$$\n",
    "\n",
    "where $P_\\mathcal{D}(c)=\\frac{\\#(c)}{|\\mathcal{D}|}$ is a probability distribution over word contexts from which negative samples are drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) showed that this objective can be equivalently written as\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} f(w,c) = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) + \\frac{k\\cdot\\#(w)\\cdot\\#(c)}{|\\mathcal{D}|} \\log \\sigma (-\\mathbf{w}^\\top\\mathbf{c}) \\right) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d},\n",
    "$$\n",
    "\n",
    "A crucial observation is that this loss function depends only on the scalar product $\\mathbf{w}^\\top\\mathbf{c}$ but not on embedding $\\mathbf{w}$ and $\\mathbf{c}$ separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Matrix factorization problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $|\\mathcal{V}|=m$, $W \\in \\mathbb{R}^{m\\times d}$ and $C \\in \\mathbb{R}^{m\\times d}$ be matrices, where each row $\\mathbf{w}\\in\\mathbb{R}^d$ of matrix $W$ is the word embedding of the corresponding word $w$ and each row $\\mathbf{c}\\in\\mathbb{R}^d$ of matrix $C$ is the context embedding of the corresponding context $c$. SGNS embeds both words and their contexts into a low-dimensional space $\\mathbb{R}^d$, resulting in the word and context matrices $W$ and $C$. The rows of matrix $W$ are typically used in NLP tasks (such as computing word similarities) while $C$ is ignored. It is nonetheless instructive to consider the product $W^\\top C = M$. Viewed this way, SGNS can be described as factorizing an implicit matrix $M$ of dimensions $m \\times m$ into two smaller matrices.\n",
    "\n",
    "Which matrix is being factorized? A matrix entry $M_{wc}$ corresponds to the dot product $\\mathbf{w}^\\top\\mathbf{c}$ . Thus, SGNS is factorizing a matrix in which each row corresponds to a word $w \\in \\mathcal{V}$ , each column corresponds to a context $c \\in \\mathcal{V}$, and each cell contains a quantity $f(w,c)$ reflecting the strength of association between that particular word-context pair. Such word-context association matrices are very common in the NLP and word-similarity literature. That said, the objective of SGNS does not explicitly state what this association metric is. What can we say about the association function $f(w,c)$? In other words, which matrix is SGNS factorizing? Below you will find the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (theoretical) 9 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve SGNS optimization problem with respect to the $\\mathbf{w}^\\top\\mathbf{c}$ and show that the matrix being factorized is\n",
    "\n",
    "$$\n",
    "M_{wc} = \\mathbf{w}^\\top\\mathbf{c} = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)} \\right)\n",
    "$$\n",
    "\n",
    "**Hint:** Denote $x=\\mathbf{w}^\\top\\mathbf{c}$, rewrite SGNG optimization problem in terms of $x$ and solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This matrix is called Shifted Pointwise Mutual Information (SPMI) matrix, as its elements can be written as\n",
    "\n",
    "$$\n",
    "\\text{SPMI}(w,c) = M_{wc} = \\mathbf{w}^\\top\\mathbf{c} = \\text{PMI}(w,c) - \\log k\n",
    "$$\n",
    "\n",
    "and $\\text{PMI}(w,c) = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{\\#(w)\\cdot\\#(c)} \\right)$ is the well-known [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) of $(w,c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log (\\frac{1}{1 + \\exp(-x)}) + \\frac{k\\cdot\\#(w)\\cdot\\#(c)}{|\\mathcal{D}|} \\log (\\frac{1}{1 + \\exp(x)}) \\right) = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) (- \\log (1 + \\exp(-x)) + \\frac{k\\cdot\\#(w)\\cdot\\#(c)}{|\\mathcal{D}|} (-\\log (1 + \\exp(x)) \\right)$\n",
    "we assume $x = (w,c)$ and take the derivative with respect to $x$:\n",
    "​\n",
    "$$\\frac{d\\mathcal{L}}{dx} = \\#(w,c) \\sigma(-x) - \\frac{k \\#(w) \\#(c) \\sigma(x)}{|D|} = 0$$\n",
    "​\n",
    "This gives us the following:\n",
    "​\n",
    "$$e^{2x} - \\Bigl(-1 + \\frac{\\#(w,c) |D|}{k \\#(w) \\#(c)}\\Bigr)e^{x} - \\frac{\\#(w,c) |D|}{k \\#(w) \\#(c)} = 0$$\n",
    "​\n",
    "This is quadratic equation, which (in terms of $y = e^{x}$) has roots $y_1 = -1$ and\n",
    "​\n",
    "$$y = \\frac{\\#(w,c) |D|}{k \\#(w) \\#(c)}$$\n",
    "​\n",
    "This immediately gives us\n",
    "​\n",
    "$$w^Tc = \\log\\Bigl(\\frac{\\#(w,c) |D|}{\\#(w) \\#(c)}\\Bigr) - \\log(k) = PMI(w,c) - \\log(k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (practical) 36 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download dataset [enwik8](http://mattmahoney.net/dc/enwik8.zip) of compressed Wikipedia articles and preprocess raw data with Perl script **main_.pl**. This script will clean all unnecessary symbols, make all words to lowercase, and produce only sentences with words. If you are Windows user, you can install [linux subsystem in windows](https://www.microsoft.com/en-us/p/ubuntu-1804-lts/9n9tngvndl3q?activetab=pivot:overviewtab) and run all these bash commands from ubuntu console (it was checked on the Windows and it works). Also, you can just download file from this [link](https://nla.skoltech.ru/homeworks/files/enwik8.txt).\n",
    "```\n",
    "wget http://mattmahoney.net/dc/enwik8.zip\n",
    "unzip enwik8.zip\n",
    "mkdir data\n",
    "perl main_.pl enwik8 > data/enwik8.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enwik 8\n",
    "\n",
    "import re\n",
    "# Download file fromhttps://nla.skoltech.ru/homeworks/files/enwik8.txt\n",
    "file = open('enwik8.txt', \"r\")\n",
    "doclist = [line for line in file]\n",
    "docstr = ''.join(doclist)\n",
    "sentences = re.split(r'[.!?]', docstr)\n",
    "sentences = [sentence.split() for sentence in sentences if len(sentence) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achilles', 'wrath', 'is', 'terrible', 'and', 'he', 'slays', 'many', 'trojan', 'warriors', 'and', 'allies', 'including', 'priam', 's', 'son', 'lycaon', 'whom', 'achilles', 'had', 'previously', 'captured', 'and', 'sold', 'into', 'slavery', 'but', 'who', 'had', 'been', 'returned', 'to', 'troy']\n"
     ]
    }
   ],
   "source": [
    "print (sentences[1249])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construct the word vocabulary from the obtained sentences which enumerates words which occur more than $r=200$ times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def create_vocabulary(sentences, r=200):\n",
    "    vocabulary = {}\n",
    "    draft_vocabulary = collections.defaultdict(int)\n",
    "    # Your code is here\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            draft_vocabulary[word] += 1\n",
    "    value = 0\n",
    "    for key in draft_vocabulary:\n",
    "        if draft_vocabulary[key] > r:\n",
    "            vocabulary.update({key: value})\n",
    "            value += 1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocabulary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5758"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scan the text corpus with sliding window of size $5$ and step $1$ (which corresponds to $L$=2) and construct co-occurrence word-context matrix $D$ with elements $D_{wc}=\\#(w,c)$. Please, ignore words which occur less than $r=200$ times, but include them into the sliding window. Please, see the graphical illustration of the procedure described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sliding window](sliding_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_matrix(sentences, vocabulary):\n",
    "    n = len(vocabulary)\n",
    "    corpus_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for sentense in sentences:\n",
    "        for p in range(len(sentense)):\n",
    "            for i in range(max(0, p - 2), min(p + 3, len(sentense))):\n",
    "                word = sentense[p]\n",
    "                context = sentense[i]\n",
    "                if (word in vocabulary and context in vocabulary) and i != p:\n",
    "                    corpus_matrix[vocabulary[word]][vocabulary[context]] += 1\n",
    "                \n",
    "    return corpus_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = create_corpus_matrix(sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To find good word embeddings, [Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) proposed to find rank-$d$ SVD of Shifted Positive Pointwise Mutual Information (SPPMI) matrix\n",
    "\n",
    "$$ U \\Sigma V^\\top \\approx \\text{SPPMI}, $$\n",
    "\n",
    "where $\\text{SPPMI}(w, c) = \\max\\left(\\text{SPMI}(w, c), 0 \\right)$ and $\\text{SPMI}(w, c)$ is the element of the matrix $\\text{SPPMI}$ at position $(w, c)$.\n",
    "Then use $W=U\\sqrt{\\Sigma}$ as word embedding matrix. Your task is to reproduce their results. Write function constructs $\\text{SPPMI}$ matrix, computes its SVD and produces word-vectors matrix $W$. Pay attention that $\\text{SPPMI}$ matrix is **sparse**!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "def compute_embeddings(D, k, d=200):\n",
    "    n = np.sum(D)\n",
    "    ss_w = D.sum(axis = 0)\n",
    "    ss_c = D.sum(axis = 1)\n",
    "    \n",
    "    u = np.array([[0 if ss_w[w] == 0 or ss_c[c] == 0 or D[w][c] == 0 else max(0,np.log((n * D[w][c])/(ss_w[w]*ss_c[c]*k))) for w in range(D.shape[0])] for c in range(D.shape[1])])\n",
    "                \n",
    "    sppmi = scipy.sparse.csc_matrix(u)\n",
    "    \n",
    "    u, sigma, v = scipy.sparse.linalg.svds(sppmi, d)\n",
    "    embedding_matrix = np.matmul(u, np.diag(np.sqrt(sigma)))   \n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5 # negative sampling parameter\n",
    "W = compute_embeddings(D, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5758, 200)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write class **WordVectors** using provided template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectors:\n",
    "    \n",
    "    def __init__(self, vocabulary, embedding_matrix):\n",
    "        self.vocab = vocabulary\n",
    "        self.W = embedding_matrix\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def word_vector(self, word):\n",
    "        if word in self.vocab:\n",
    "            word_vector = self.W[vocab[word]]\n",
    "        else:\n",
    "            raise ValueError('There is no such word in vocab')\n",
    "\n",
    "        return word_vector\n",
    "    \n",
    "    def nearest_words(self, word, top_n=10):\n",
    "        v = self.word_vector(word)\n",
    "        raw_result= W@v/np.linalg.norm(v)/np.linalg.norm(W, axis=1)\n",
    "        arguments = np.argsort(raw_result)\n",
    "        neighbors = [(self.inv_vocab[i],raw_result[i]) for i in arguments[-11:-1]]\n",
    "        return neighbors[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordVectors(vocab, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Calculate top 10 nearest neighbours with the corresponding cosine similarities for the words {matrix, multiplication, algorithm} and print them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('communism', 0.7848589121153872),\n",
       " ('anarcho', 0.783242473461076),\n",
       " ('capitalism', 0.7808426453313528),\n",
       " ('socialism', 0.7438494779661052),\n",
       " ('liberalism', 0.7098890553594412),\n",
       " ('criticisms', 0.7079910605546905),\n",
       " ('capitalist', 0.6698667056521742),\n",
       " ('fascism', 0.5474056803961747),\n",
       " ('anarchist', 0.5184369607228633),\n",
       " ('nationalism', 0.511902518841822)]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukraine', 0.6540922172595167),\n",
       " ('russia', 0.6148685286643405),\n",
       " ('belarus', 0.5468665605987431),\n",
       " ('poland', 0.5437339705491117),\n",
       " ('yugoslavia', 0.5244773507315705),\n",
       " ('romania', 0.5062120368294338),\n",
       " ('austria', 0.49593649550725105),\n",
       " ('serbia', 0.49190831514590794),\n",
       " ('hungary', 0.4597797223505655),\n",
       " ('finland', 0.41586903441142303)]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"ussr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hop', 0.8253395849004908),\n",
       " ('hip', 0.8105137935853014),\n",
       " ('funk', 0.7448464342399237),\n",
       " ('rock', 0.7314108756121545),\n",
       " ('punk', 0.7103397120873213),\n",
       " ('music', 0.6878431146198494),\n",
       " ('band', 0.6601902935396652),\n",
       " ('pop', 0.6593059204986254),\n",
       " ('scene', 0.6569535675366406),\n",
       " ('jazz', 0.6260219107813182)]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"rap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 (eigenvalues)  45 pts\n",
    "\n",
    "### 1. Theoretical tasks (10 pts)\n",
    "\n",
    "* (5 pts) Prove that normal matrix is Hermitian iff its eigenvalues are real. Prove that normal matrix is unitary iff its eigenvalues satisfy $|\\lambda| = 1$. \n",
    "\n",
    "* (5 pts) The following problem illustrates instability of the Jordan form. Find theoretically the eigenvalues of the perturbed Jordan block:\n",
    "\n",
    "$$\n",
    "    J(\\varepsilon) = \n",
    "    \\begin{bmatrix} \n",
    "     \\lambda & 1 & & & 0 \\\\ \n",
    "     & \\lambda & 1 & & \\\\ \n",
    "     &  & \\ddots & \\ddots & \\\\ \n",
    "     & & & \\lambda & 1 \\\\ \n",
    "     \\varepsilon & & & & \\lambda  \\\\ \n",
    "    \\end{bmatrix}_{n\\times n}\n",
    "$$\n",
    "\n",
    "Comment how eigenvalues of $J(0)$ are perturbed for large $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1\n",
    "- If eigenvalues are real, A is normal matrix. \n",
    "\n",
    "*Proposition.* A matrix A is normal if and only if there exists a diagonal matrix $\\Lambda$ and a unitary matrix U such that $A = U \\Lambda U^*$.\n",
    "\n",
    "$A^* = (U \\Lambda U^*)^* = U (U \\Lambda)^* = U \\Lambda^* U^* = U \\Lambda U^* = A$, as $\\Lambda = \\Lambda^*$ - the diagonal matrix with real eigenvalues of A.\n",
    "\n",
    "If A is a normal Hermitian matrix, consequently eigenvalues of matrix A are real, because it's the property of Hermitian matrices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2\n",
    "\n",
    "$Ax = \\lambda x \\\\\n",
    "A^*x = \\lambda^* x \\\\\n",
    " => A^{*}Ax = \\lambda A^{*}x => A^{*}Ax = \\lambda \\lambda^{*}x = |\\lambda|^{2}x \\\\\n",
    "\\text { A is unitary} => A^{*}A = I \\\\\n",
    "Ix = |\\lambda|^{2}x  \\text{ for all x} => |\\lambda| = 1 \\\\\n",
    "\\text{Lets write spectrum decomposition:} \\\\\n",
    "A^{*}A = (UDU^{*})^{*}(UDU^{*}) = UD^{*}U^{*}UDU^{*} = \\text{(U is unitary)} = UD^{*}DU^{*} = (|\\lambda| = 1) = UIU^{*} = I$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3\n",
    "\n",
    "For Jordan block eigenvalues we have to calculate determinant of:\n",
    "\n",
    "$\n",
    "J(\\varepsilon) - \\lambda_1 I = \n",
    "    \\begin{bmatrix} \n",
    "     \\lambda - \\mu  & 1 & & & 0 \\\\ \n",
    "     & \\lambda - \\mu & 1 & & \\\\ \n",
    "     &  & \\ddots & \\ddots & \\\\ \n",
    "     & & & \\lambda - \\mu & 1 \\\\ \n",
    "     \\varepsilon & & & & \\lambda - \\mu  \\\\ \n",
    "    \\end{bmatrix}_{n\\times n}\n",
    "$\n",
    "\n",
    "This gives:\n",
    "\n",
    "\n",
    "$det(J(\\epsilon) - \\mu I) = (\\lambda - \\mu)^{n} + (-1)^{n+1}\\varepsilon = 0$,\n",
    "\n",
    "$\\mu = \\lambda - \\sqrt[n]{((-1)^n \\varepsilon)}$, where it is n-th root of complex number.\n",
    "\n",
    "Since $J(0)$ is a simple Jordan block, its eigenvalues are $\\lambda$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PageRank\n",
    "\n",
    "\n",
    "#### Damping factor importance\n",
    "\n",
    "* (5 pts) Write the function ```pagerank_matrix(G)``` that takes an adjacency matrix $G$ (in both sparse and dense formats) as an input and outputs the corresponding PageRank matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT:  G - np.ndarray or sparse matrix\n",
    "# OUTPUT: A - np.ndarray (of size G.shape) or sparse matrix\n",
    "def pagerank_matrix(G):\n",
    "    v = np.sum(G, axis = 0)\n",
    "    v =np.array([i if i!=0 else 1 for i in v])\n",
    "    A = G/v\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT: A - np.ndarray (of size G.shape) or sparse matrix\n",
    "import scipy.sparse as sparse\n",
    "def pagerank_matrix(G):\n",
    "    if (sparse.issparse(G)):\n",
    "        v = np.sum(G, axis = 1)\n",
    "        v = np.where(v==0,1,v)\n",
    "        D = sparse.diags(np.squeeze(np.asarray(1/v)))\n",
    "        return G.T @ D\n",
    "    else:\n",
    "        v = np.sum(G, axis = 0)\n",
    "        v =[v if v!=0 else 1 for v in v]\n",
    "        A = G/v\n",
    "        return A\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (3 pts) Find PageRank matrix $A$ that corresponds to the following graph: \n",
    "![graph](graph.png)\n",
    "What is its largest eigenvalue? What multiplicity does it have?\n",
    "\n",
    "\n",
    "* (5 pts) Implement the power method for a given matrix $A$, an initial guess $x_0$ and a number of iterations ```num_iter```. It should be organized as a function ```power_method(A, x0, num_iter)``` that outputs approximation to eigenvector $x$, eigenvalue $\\lambda$ and history of residuals $\\{\\|Ax_k - \\lambda_k x_k\\|_2\\}$. Make sure that the method converges to the correct solution on a matrix $\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$ which is known to have the largest eigenvalue equal to $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = np.array([[0,1,0,0,0],[0,0,1,0,0],[1,1,0,0,0],[0,0,0,0,1],[0,0,0,1,0]]).T\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pagerank_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0.5, 0. , 0. ],\n",
       "       [1. , 0. , 0.5, 0. , 0. ],\n",
       "       [0. , 1. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 1. ],\n",
       "       [0. , 0. , 0. , 1. , 0. ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT:  A - np.ndarray (2D), x0 - np.ndarray (1D), num_iter - integer (positive) \n",
    "# OUTPUT: x - np.ndarray (of size x0), l - float, res - np.ndarray (of size num_iter + 1 [include initial guess])\n",
    "def power_method(A, x0, num_iter): # 5 pts\n",
    "    x_old = x0\n",
    "    res = []\n",
    "    for i in range(num_iter):\n",
    "        x_new = A@x_old\n",
    "        x_old = x_new/np.linalg.norm(x_new, ord = 2)\n",
    "        lambd_k = x_old@(A@x_old)\n",
    "        _res = np.linalg.norm(A@x_old-lambd_k*x_old, ord = 2)\n",
    "        res.append(_res)\n",
    "    res = np.array(res) \n",
    "    lambd = ((A@x_old).T)@x_old\n",
    "    return x_old, lambd, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "T = np.array([[2,-1], [-1,2]])\n",
    "_,l,_ = power_method(T, (-5,-2), 100)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,l,res = power_method(A, np.random.random_sample(5), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65842354, 0.66445276, 0.65650832, 0.65943607, 0.66005993,\n",
       "       0.65768262, 0.65945429, 0.65874426, 0.6585422 , 0.65908115,\n",
       "       0.65862487, 0.65880374, 0.65885137, 0.65871317, 0.65882641,\n",
       "       0.65878177, 0.65876969, 0.65880402, 0.65877566, 0.65878682,\n",
       "       0.65878983, 0.65878124, 0.65878832, 0.65878553, 0.65878478,\n",
       "       0.65878693, 0.65878516, 0.65878585, 0.65878604, 0.6587855 ,\n",
       "       0.65878595, 0.65878577, 0.65878573, 0.65878586, 0.65878575,\n",
       "       0.65878579, 0.65878581, 0.65878577, 0.6587858 , 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579,\n",
       "       0.65878579, 0.65878579, 0.65878579, 0.65878579, 0.65878579])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (2 pts) Run the power method for the graph presented above and plot residuals $\\|Ax_k - \\lambda_k x_k\\|_2$ as a function of $k$ for ```num_iter=100``` and random initial guess ```x0```.  Explain the absence of convergence. \n",
    "\n",
    "\n",
    "* (2 pts) Consider the same graph, but with a directed edge that goes from the node 3 to the node 4 being removed. Plot residuals as in the previous task and discuss the convergence. Now, run the power method with ```num_iter=100``` for 10 different initial guesses and print/plot the resulting approximated eigenvectors. Why do they depend on the initial guess?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21032657860>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. +0.j ,  1. +0.j , -0.5+0.5j, -0.5-0.5j, -1. +0.j ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.linalg.eig(A)[0])[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence ratio is $q^k$, where $q = \\left|\\frac{\\lambda_{2}}{\\lambda_{1}}\\right| < 1$, for $\\lambda_1>\\lambda_2\\geq\\dots\\geq \\lambda_n$ and $k$ is the number of iteration. But here $\\lambda_{1} = \\lambda_{2}$, q = 1 => there is no convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove one edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_new = np.array([[0,1,0,0,0],[0,0,1,0,0],[1,1,0,0,0],[0,0,0,0,1],[0,0,0,1,1]]).T\n",
    "G_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_new = pagerank_matrix(G_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0.5, 0. , 0. ],\n",
       "       [1. , 0. , 0.5, 0. , 0. ],\n",
       "       [0. , 1. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0.5],\n",
       "       [0. , 0. , 0. , 1. , 0.5]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,l,res = power_method(A_new, np.random.random_sample(5), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x210327111d0>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. +0.j ,  1. +0.j , -0.5+0.5j, -0.5+0.j , -0.5-0.5j])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.linalg.eig(A_new)[0])[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see $\\lambda_1$ and  $\\lambda_2$ here are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.04816258  1.26433775  0.69242669 -0.3389469   1.12635296] [0.3169629  0.63392579 0.63392579 0.13842738 0.27685476]\n",
      "[-1.32421256  1.16729158  0.43287782  1.63263366 -0.19870614] [0.05103056 0.10206112 0.10206112 0.44194184 0.88388367]\n",
      "[-0.42478298 -0.42985998 -0.92906825  0.92904069 -0.02215987] [-0.28182769 -0.56365537 -0.56365537  0.23881306  0.47762613]\n",
      "[-1.977005   -1.3082576   0.99665624  0.05886222 -0.38447055] [-0.32824599 -0.65649199 -0.65649199 -0.07783458 -0.15566916]\n",
      "[ 1.5240456   0.3604421  -1.49080007  0.63670378  0.61860451] [0.08159262 0.16318525 0.16318525 0.43360899 0.86721799]\n",
      "[-0.70855037  0.27147164  1.71578714  0.40686663  0.50360914] [0.24967691 0.49935382 0.49935382 0.29629478 0.59258955]\n",
      "[ 0.40468824 -1.14736191 -0.72231692 -1.09513031 -0.33553857] [-0.21201989 -0.42403979 -0.42403979 -0.34508784 -0.69017568]\n",
      "[ 0.62347722 -0.75031382 -0.73986307 -1.07477189 -1.28812446] [-0.09439288 -0.18878576 -0.18878576 -0.42890788 -0.85781576]\n",
      "[-1.41126681 -0.81546548 -1.1407693  -0.37202899  1.81189131] [-0.29438284 -0.58876568 -0.58876568  0.20978498  0.41956996]\n",
      "[-0.0656818  -0.57528776 -0.27453008 -1.28480716 -0.58933915] [-0.12198323 -0.24396647 -0.24396647 -0.41619246 -0.83238492]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    x0= np.random.randn(5)\n",
    "    x, _, _ = power_method(A_new, x0, 100)\n",
    "    print(x0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### They are differen because multiplicity of the largest eigenvalue is 2 (multiple eigenvalues correspond to one SET of collinear eigenvectors - there is ambiguity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### They are differen because eigenvectors are determined up to a constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In order to avoid this problem Larry Page and Sergey Brin [proposed](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf) to use the following regularization technique:\n",
    "\n",
    "$$\n",
    "A_d = dA + \\frac{1-d}{N} \\begin{pmatrix} 1 & \\dots & 1 \\\\ \\vdots & & \\vdots \\\\ 1 & \\dots & 1 \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where $d$ is a small parameter in $[0,1]$ (typically $d=0.85$), which is called **damping factor**, $A$ is of size $N\\times N$. Now $A_d$ is the matrix with multiplicity of the largest eigenvalue equal to 1. \n",
    "Recall that computing the eigenvector of the PageRank matrix, which corresponds to the largest eigenvalue, has the following interpretation. Consider a person who stays in a random node of a graph (i.e. opens a random web page); at each step s/he follows one of the outcoming edges uniformly at random (i.e. opens one of the links). So the person randomly walks through the graph and the eigenvector we are looking for is exactly his/her stationary distribution â€” for each node it tells you the probability of visiting this particular node. Therefore, if the person has started from a part of the graph which is not connected with the other part, he will never get there.  In the regularized model, the person at each step follows one of the outcoming links with probability $d$ OR teleports to a random node from the whole graph with probability $(1-d)$.\n",
    "\n",
    "* (2 pts) Now, run the power method with $A_d$ and plot residuals $\\|A_d x_k - \\lambda_k x_k\\|_2$ as a function of $k$ for $d=0.97$, ```num_iter=100``` and a random initial guess ```x0```.\n",
    "\n",
    "* (5 pts) Find the second largest in the absolute value eigenvalue of the obtained matrix $A_d$. How and why is it connected to the damping factor $d$? What is the convergence rate of the PageRank algorithm when using damping factor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0.97\n",
    "N =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_d = d*A + ((1-d)/N)*np.ones_like(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.006, 0.006, 0.491, 0.006, 0.006],\n",
       "       [0.976, 0.006, 0.491, 0.006, 0.006],\n",
       "       [0.006, 0.976, 0.006, 0.006, 0.006],\n",
       "       [0.006, 0.006, 0.006, 0.006, 0.976],\n",
       "       [0.006, 0.006, 0.006, 0.976, 0.006]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,l,res = power_method(A_d, (1,2,3,1,1), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2265a033f98>]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999777350111719"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let $B = A- \\lambda_i v_ix^{T} $ where $v_i^{T}x=1$. Then eigenvalues of B are 0, and $\\lambda_j$ for $j \\neq i$. Proof: $Bv_i = Av_i-\\lambda_i v_ix^Tv_i = (\\lambda_i - \\lambda_i(1))v_i=0 \\\\\n",
    "\\text{for } j \\neq i, B^Tw_i=(A-\\lambda_iv_ix^T)^Tw_j = A^Tw_j-\\lambda_ix(v_i^Tw_i) = A^Tw_i=\\lambda_jw_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = x*np.ones_like(x)/(np.linalg.norm(x, ord = 2)).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A_d - l*x*h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06637444, -0.2749771 ,  0.21277817, -0.17821331, -0.17821331],\n",
       "       [ 0.90362556, -0.2749771 ,  0.21277817, -0.17821331, -0.17821331],\n",
       "       [-0.06637444,  0.6950229 , -0.27222183, -0.17821331, -0.17821331],\n",
       "       [-0.06637444, -0.2749771 , -0.27222183, -0.17821331,  0.79178669],\n",
       "       [-0.06637444, -0.2749771 , -0.27222183,  0.79178669, -0.17821331]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97      +0.j        , -0.36193584+0.51230793j,\n",
       "       -0.36193584-0.51230793j, -0.24612832+0.j        ,\n",
       "       -0.97      +0.j        ])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(B)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, l, _  = power_method(B, (1,2,3,1,1), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, second eigenvalue of A_d is d - theoretical proof of this fact:\n",
    "http://www-cs-students.stanford.edu/~taherh/papers/secondeigenvalue.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, graphs that arise in various areas are sparse (social, web, road networks, etc.) and, thus, computation of a matrix-vector product for corresponding PageRank matrix $A$ is much cheaper than $\\mathcal{O}(N^2)$. However, if $A_d$ is calculated directly, it becomes dense and, therefore, $\\mathcal{O}(N^2)$ cost grows prohibitively large for  big $N$.\n",
    "\n",
    "\n",
    "* (2 pts) Implement fast matrix-vector product for $A_d$ as a function ```pagerank_matvec(A, d, x)```, which takes a PageRank matrix $A$ (in sparse format, e.g., ```csr_matrix```), damping factor $d$ and a vector $x$ as an input and returns $A_dx$ as an output. \n",
    "\n",
    "* (1 pts) Generate a random adjacency matrix of size $10000 \\times 10000$ with only 100 non-zero elements and compare ```pagerank_matvec``` performance with direct evaluation of $A_dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_matvec(A, d, x): # 2 pts\n",
    "    # enter your code here\n",
    "    N = A.shape[0]\n",
    "    sum_ = np.sum(x)\n",
    "    y = d * A@x + (1-d)/N * sum_\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n",
      "Naive 0.09395320000112406\n",
      "Matvec 0.011525800000526942\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import random as rand_mat\n",
    "import time \n",
    "\n",
    "\n",
    "k = 100\n",
    "n = 10000\n",
    "A = rand_mat(n, n, density=k/(n*n), format='csr', dtype='float32')\n",
    "print(A.shape)\n",
    "Ad = d * A + (1 - d)/n * np.ones((n,n))\n",
    "x0 = np.random.rand(A.shape[0], 1)\n",
    "\n",
    "start_time1 = time.perf_counter()\n",
    "y = pagerank_matvec(A, d, x0)\n",
    "matvec_time = time.perf_counter() - start_time1\n",
    "\n",
    "start_time2 = time.perf_counter()\n",
    "y = Ad@x0\n",
    "naive_time = time.perf_counter()  - start_time2\n",
    "\n",
    "print(f'Naive {naive_time}')\n",
    "print(f'Matvec {matvec_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We do smart computation using fact that A is sparse. We split out computation on 2 parts: with sparse matrix and with simple full of ones matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBLP: computer science bibliography\n",
    "\n",
    "Download the dataset from [here](https://goo.gl/oZVxEa), unzip it and put `dblp_authors.npz`  and `dblp_graph.npz` in the same folder with this notebook. Each value (author name) from `dblp_authors.npz` corresponds to the row/column of the matrix from `dblp_graph.npz`. Value at row `i` and column `j` of the matrix from `dblp_graph.npz` corresponds to the number of times author `i` cited papers of the author `j`. Let us now find the most significant scientists according to PageRank model over DBLP data.\n",
    "\n",
    "* (4 pts) Load the weighted adjacency matrix and the authors list into Python using ```load_dblp(...)``` function. Print its density (fraction of nonzero elements). Find top-10 most cited authors from the weighted adjacency matrix. Now, make all the weights of the adjacency matrix equal to 1 for simplicity (consider only existence of connection between authors, not its weight). Obtain the PageRank matrix $A$ from the adjacency matrix and verify that it is stochastic.\n",
    " \n",
    " \n",
    "* (1 pts) In order to provide ```pagerank_matvec``` to your ```power_method``` (without rewriting it) for fast calculation of $A_dx$, you can create a ```LinearOperator```: \n",
    "```python\n",
    "L = scipy.sparse.linalg.LinearOperator(A.shape, matvec=lambda x, A=A, d=d: pagerank_matvec(A, d, x))\n",
    "```\n",
    "Calling ```L@x``` or ```L.dot(x)``` will result in calculation of ```pagerank_matvec(A, d, x)``` and, thus, you can plug $L$ instead of the matrix $A$ in the ```power_method``` directly. **Note:** though in the previous subtask graph was very small (so you could disparage fast matvec implementation), here it is very large (but sparse), so that direct evaluation of $A_dx$ will require $\\sim 10^{12}$ matrix elements to store - good luck with that (^_<).\n",
    "\n",
    "\n",
    "* (2 pts) Run the power method starting from the vector of all ones and plot residuals $\\|A_dx_k - \\lambda_k x_k\\|_2$  as a function of $k$ for $d=0.85$.\n",
    "\n",
    "\n",
    "* (1 pts) Print names of the top-10 authors according to PageRank over DBLP when $d=0.85$. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "def load_dblp(path_auth, path_graph):\n",
    "    G = load_npz(path_graph).astype(float)\n",
    "    with np.load(path_auth) as data: authors = data['authors']\n",
    "    return G, authors\n",
    "G, authors = load_dblp('dblp_authors.npz', 'dblp_graph.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766547, 1766547)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.sum(G, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195923"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraction of nonzero elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6769834032154254"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(n)/G.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.asarray(np.argsort(n))[0][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David G. Lowe 335649\n",
      "David E. Culler 334688\n",
      "Ion Stoica 633904\n",
      "Jitendra Malik 732480\n",
      "Cordelia Schmid 293002\n",
      "Anil K. Jain 108524\n",
      "Jiawei Han 720105\n",
      "Hari Balakrishnan 563408\n",
      "Andrew Zisserman 101227\n",
      "Scott Shenker 1391413\n"
     ]
    }
   ],
   "source": [
    "for i in top:\n",
    "    print(authors[int(i)], int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_ones = (G.astype(bool)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "A = pagerank_matrix(G_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05555556, 0.05555556, 0.05555556, ..., 0.09090909, 0.09090909,\n",
       "       0.09090909])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that marlix is left stochastic(if you don't notice zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 748133, 0.0: 313520, 0.9999999999999998: 290426, 0.9999999999999999: 164977, 1.0000000000000002: 164549, 0.9999999999999997: 48205, 0.9999999999999996: 23729, 1.0000000000000004: 9650, 0.9999999999999994: 2177, 0.9999999999999993: 1181})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "s = np.asarray(A.sum(axis = 0))\n",
    "cnt = Counter(np.squeeze(s))\n",
    "print(cnt)\n",
    "eps = 10 ** (-10)\n",
    "((abs(s-1) < eps).astype(int) + (abs(s) <eps ).astype(int)).all() #Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = linalg.LinearOperator(A.shape, matvec=lambda x, A=A, d=d: pagerank_matvec(A, d, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1766547"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,l,res = power_method(L, np.random.random_sample(L.shape[0]), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22659e21c50>]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 505969, 1313064,   66090, 1516121,  108524, 1330562,  151524,\n",
       "        745295, 1304778,  334728], dtype=int64)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(x)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerard Salton 505969\n",
      "Robert Endre Tarjan 1313064\n",
      "Alfred V. Aho 66090\n",
      "Takeo Kanade 1516121\n",
      "Anil K. Jain 108524\n",
      "Ronald L. Rivest 1330562\n",
      "Azriel Rosenfeld 151524\n",
      "John E. Hopcroft 745295\n",
      "Richard M. Karp 1304778\n",
      "David E. Goldberg 334728\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(x)[-10:]:\n",
    "    \n",
    "    print(authors[int(i)], int(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
